# Mekhanikube ğŸ”§

**Your Kubernetes AI Mechanic**

AnÃ¡lise inteligente de clusters Kubernetes usando K8sGPT com IA local (Ollama). Diagnostica problemas, explica causas e sugere soluÃ§Ãµes automaticamente.

## ğŸš€ Quick Start

```powershell
# 1. Subir os serviÃ§os (Ollama + K8sGPT)
docker-compose up -d

# 2. Aguardar containers iniciarem
Start-Sleep -Seconds 5

# 3. Baixar o modelo Gemma (apenas primeira vez - ~5GB)
docker exec mekhanikube-ollama ollama pull gemma:7b

# 4. Analisar cluster com explicaÃ§Ãµes da IA (configuraÃ§Ã£o Ã© automÃ¡tica!)
docker exec mekhanikube-k8sgpt k8sgpt analyze --explain
```

## ğŸ“‹ Comandos K8sGPT

```powershell
# Analisar cluster (sem IA)
docker exec mekhanikube-k8sgpt k8sgpt analyze

# Analisar com explicaÃ§Ãµes da IA
docker exec mekhanikube-k8sgpt k8sgpt analyze --explain

# Analisar namespace especÃ­fico
docker exec mekhanikube-k8sgpt k8sgpt analyze -n kube-system --explain

# Filtrar por tipo de recurso
docker exec mekhanikube-k8sgpt k8sgpt analyze --filter=Pod --explain
docker exec mekhanikube-k8sgpt k8sgpt analyze --filter=Service --explain

# Listar filtros disponÃ­veis
docker exec mekhanikube-k8sgpt k8sgpt filters list

# Verificar configuraÃ§Ã£o
docker exec mekhanikube-k8sgpt k8sgpt auth list
```

## ğŸ› ï¸ ConfiguraÃ§Ã£o

### Modelos Ollama Recomendados

```powershell
# Gemma 7B (recomendado - boa qualidade)
docker exec mekhanikube-ollama ollama pull gemma:7b

# Mistral (alternativa)
docker exec mekhanikube-ollama ollama pull mistral

# TinyLlama (mais rÃ¡pido, qualidade inferior)
docker exec mekhanikube-ollama ollama pull tinyllama
```

### Trocar modelo

```powershell
# Remover backend atual
docker exec mekhanikube-k8sgpt k8sgpt auth remove --backend localai

# Adicionar com novo modelo
docker exec mekhanikube-k8sgpt k8sgpt auth add --backend localai --model mistral --baseurl http://localhost:11434/v1
docker exec mekhanikube-k8sgpt k8sgpt auth default -p localai
```

## ğŸ“Š Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Kubernetes    â”‚
â”‚     Cluster     â”‚
â”‚   (em VM/Host)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ kubeconfig (montado em /root/.kube/)
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  k8sgpt container â”‚
    â”‚  - Ajusta config  â”‚
    â”‚    automaticamenteâ”‚
    â”‚  - Roda anÃ¡lises  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ API calls (http://localhost:11434/v1)
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ollama container  â”‚
    â”‚  - Gemma:7b model â”‚
    â”‚  - Gera explicaÃ§Ãµesâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Troubleshooting

### K8sGPT nÃ£o consegue acessar cluster

```powershell
# Verificar se kubeconfig estÃ¡ montado
docker exec mekhanikube-k8sgpt ls -la /root/.kube/

# Verificar se config_mod foi criado pelo entrypoint
docker exec mekhanikube-k8sgpt cat /root/.kube/config_mod

# Testar conexÃ£o manual
docker exec mekhanikube-k8sgpt kubectl get nodes
```

### Ollama nÃ£o responde

```powershell
# Ver logs
docker logs mekhanikube-ollama

# Verificar modelos instalados
docker exec mekhanikube-ollama ollama list

# Testar API
Invoke-RestMethod -Uri http://localhost:11434/v1/models | ConvertTo-Json

# Baixar modelo novamente
docker exec mekhanikube-ollama ollama pull gemma:7b
```

### Container k8sgpt nÃ£o inicia

```powershell
# Ver logs
docker logs mekhanikube-k8sgpt

# Reconstruir imagem
docker-compose build k8sgpt
docker-compose up -d k8sgpt
```

## ğŸ“š Recursos

- [K8sGPT Docs](https://docs.k8sgpt.ai/)
- [Ollama Models](https://ollama.com/library)
- [K8sGPT GitHub](https://github.com/k8sgpt-ai/k8sgpt)

## ğŸ” Como Funciona

1. **Entrypoint automÃ¡tico**: O container k8sgpt executa `/entrypoint.sh` ao iniciar, que:
   - Copia o kubeconfig montado de `/root/.kube/config`
   - Substitui `127.0.0.1` por `host.docker.internal` para acessar o cluster na VM/Host
   - Salva em `/root/.kube/config_mod`
   - Define `KUBECONFIG=/root/.kube/config_mod`

2. **AnÃ¡lise**: K8sGPT escaneia o cluster e identifica problemas (ConfigMaps nÃ£o usados, Pods com erro, etc)

3. **ExplicaÃ§Ã£o**: Quando usa `--explain`, K8sGPT envia o problema para Ollama via API REST

4. **Resposta**: Ollama processa com o modelo gemma:7b e retorna explicaÃ§Ã£o + soluÃ§Ã£o



Se vocÃª jÃ¡ tem Ollama rodando:export OLLAMA_MODEL=mistral



```powershell```2. Inicie o Ollama:

# O programa detecta automaticamente

.\kube-ai.exe```bash

```

## Usodocker-compose up -d

**Nota:** Ollama Ã© significativamente mais lento (1-2 minutos por scan).

```

---

```bash

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

# Iniciar chat interativo3. Instale o modelo Mistral:

### VariÃ¡veis de Ambiente

./kube-ai```bash

```powershell

# ForÃ§ar uso de LocalAIdocker exec -it ollama ollama pull mistral

$env:LLM_PROVIDER="localai"

$env:LOCALAI_URL="http://localhost:8080"# Comandos disponÃ­veis:```

$env:LOCALAI_MODEL="phi-2"

# scan    - Escanear cluster em busca de problemas

# ForÃ§ar uso de Ollama

$env:LLM_PROVIDER="ollama"# exit    - Sair do chat4. Compile e instale a CLI:

$env:OLLAMA_URL="http://localhost:11434"

$env:OLLAMA_MODEL="mistral"# qualquer texto - Fazer perguntas sobre Kubernetes```bash

```

```go install ./cmd/kube-ai

---

```

## ğŸ“Š ComparaÃ§Ã£o de Performance

## Exemplos

| Provider | Modelo    | Tempo/Scan | Qualidade | RAM   |

|----------|-----------|------------|-----------|-------|## Uso

| LocalAI  | phi-2     | ~5-10s     | â­â­â­â­    | 2GB   |

| Ollama   | mistral   | ~60-120s   | â­â­â­â­â­  | 4GB   |```

| Ollama   | tinyllama | ~30-60s    | â­â­â­     | 2GB   |

> scanSimplesmente execute:

**RecomendaÃ§Ã£o:** Use LocalAI com phi-2 para melhor balance entre velocidade e qualidade.

ğŸ” Escaneando cluster...```bash

---

ğŸ¤– Analisando 2 problemas encontrados...kube-ai

## ğŸ› ï¸ Troubleshooting

```

### LocalAI nÃ£o inicia

> O que Ã© um CrashLoopBackOff?

```powershell

# Verifique se o modelo foi baixadoğŸ¤– CrashLoopBackOff indica que um container estÃ¡ falhando...A ferramenta irÃ¡:

dir .\models\

1. Conectar ao seu cluster Kubernetes

# Verifique logs do container

docker-compose logs localai> Como debugar um pod?2. Procurar por pods com problemas



# Reinicie o serviÃ§oğŸ¤– Use kubectl describe pod <name> para ver eventos...3. Coletar informaÃ§Ãµes detalhadas

docker-compose restart

``````4. Usar IA local para analisar e sugerir soluÃ§Ãµes



### Scan muito lento

Se nenhum problema for encontrado, vocÃª verÃ¡:

- âœ… **SoluÃ§Ã£o:** Use LocalAI em vez de Ollama```

- Execute: `.\download-model.ps1` e `docker-compose up -d`âœ… Cluster saudÃ¡vel

```

### Erro de conexÃ£o com Kubernetes

Se problemas forem encontrados, vocÃª receberÃ¡ uma anÃ¡lise detalhada com:

```powershell- Causa provÃ¡vel do problema

# Verifique se o cluster estÃ¡ acessÃ­vel- Como resolver o problema

kubectl cluster-info- Como prevenir que aconteÃ§a novamente

go mod init kube-ai

# Verifique o contexto atualgo get k8s.io/client-go

kubectl config current-contextgo build -o kube-ai ./cmd/kube-ai

``````



---## Uso



## ğŸ“¦ Requisitos```bash

./kube-ai

- **Go:** 1.21 ou superior```

- **Docker Desktop:** Com Kubernetes habilitado

- **RAM:** 4GB disponÃ­vel## Estrutura do Projeto

- **Disco:** 2GB para modelo Phi-2

```

---kube-ai/

 â”œâ”€â”€ cmd/

## ğŸ—ï¸ Arquitetura â”‚    â””â”€â”€ kube-ai/        # main.go, parsing de comandos CLI

 â”œâ”€â”€ internal/

``` â”‚    â”œâ”€â”€ k8s/            # conexÃ£o + scanner

kube-ai/ â”‚    â”‚    â”œâ”€â”€ connect.go

â”œâ”€â”€ cmd/kube-ai/          # CLI principal â”‚    â”‚    â””â”€â”€ scan.go

â”œâ”€â”€ internal/ â”‚    â”œâ”€â”€ llm/            # integraÃ§Ã£o com ollama

â”‚   â”œâ”€â”€ k8s/             # Cliente Kubernetes â”‚    â”‚    â””â”€â”€ ollama.go

â”‚   â””â”€â”€ llm/             # Cliente LLM (LocalAI/Ollama) â”‚    â””â”€â”€ explain/        # heurÃ­sticas e montagem de prompts

â”œâ”€â”€ models/              # Modelos de IA â”‚         â””â”€â”€ explain.go

â”œâ”€â”€ docker-compose.yml   # LocalAI setup â”œâ”€â”€ go.mod

â””â”€â”€ download-model.ps1   # Script para baixar Phi-2 â””â”€â”€ README.md

``````

---

## ğŸ“ LicenÃ§a

MIT


