# Perguntas Frequentes (FAQ)

## Quest√µes Gerais

### O que √© o NautiKube?

NautiKube √© uma solu√ß√£o containerizada **pr√≥pria** desenvolvida em Go que analisa clusters Kubernetes e fornece explica√ß√µes via IA local (Ollama). Vers√£o 2.0 traz engine customizado que substitui o K8sGPT por uma solu√ß√£o 60% mais leve e 3x mais r√°pida.

### Qual a diferen√ßa entre v1 e v2?

**NautiKube v2.0 (atual)**:
- ‚úÖ Engine pr√≥prio em Go (1.618 linhas)
- ‚úÖ Imagem ~80MB (60% menor)
- ‚úÖ Startup <10s (3x mais r√°pido)
- ‚úÖ Configura√ß√£o autom√°tica (zero setup)
- ‚úÖ Suporte nativo ao portugu√™s

**K8sGPT (v1 - legado)**:
- Ferramenta externa
- Imagem ~200MB
- Startup ~30s
- Requer configura√ß√£o manual
- Dispon√≠vel via `--profile k8sgpt`

### Por que "NautiKube"?

**Nauti** (Grego: ŒΩŒ±œÖœÑŒπŒ∫œåœÇ) = n√°utico/navegador + **kube** (Kubernetes) = Seu navegador Kubernetes!

O nome reflete a natureza da ferramenta: um explorador/navegador de diagn√≥sticos, n√£o um reparador. Alinha-se com a tem√°tica n√°utica do Kubernetes (kubernetes = timoneiro em grego).

### √â gratuito?

Sim! NautiKube √© c√≥digo aberto sob a Licen√ßa MIT. Ollama tamb√©m √© gratuito e de c√≥digo aberto.

### Ele envia meus dados para algum lugar?

N√£o! Tudo roda 100% localmente na sua m√°quina. Os dados do seu cluster nunca saem da sua infraestrutura. Sem telemetria, sem chamadas de API externas.

---

## Instala√ß√£o & Configura√ß√£o

### Quais s√£o os requisitos do sistema?

**M√≠nimo (v2.0)**:
- Docker & Docker Compose
- 1 n√∫cleo de CPU
- 2GB RAM
- 5GB de espa√ßo em disco
- Cluster Kubernetes ativo

**Recomendado**:
- 2-4 n√∫cleos de CPU
- 4-8GB RAM
- 10GB de espa√ßo em disco (m√∫ltiplos modelos)

### Quais sistemas operacionais s√£o suportados?

- ‚úÖ Windows 10/11 (com Docker Desktop)
- ‚úÖ macOS (Intel & Apple Silicon)
- ‚úÖ Linux (qualquer distribui√ß√£o com Docker)

### Posso usar com qualquer cluster Kubernetes?

Sim! NautiKube funciona com:
- Clusters locais (Docker Desktop, Minikube, Kind)
- Clusters na nuvem (EKS, GKE, AKS)
- Clusters on-premise
- Qualquer cluster acess√≠vel via kubeconfig

### Quanto tempo leva a configura√ß√£o?

**NautiKube v2.0**:
- Primeira vez: ~10-15 minutos (incluindo download do modelo)
- Inicializa√ß√µes subsequentes: <10 segundos
- Mudan√ßas de modelo: ~5-10 minutos por modelo

**K8sGPT (legado)**:
- Primeira vez: ~15-20 minutos
- Inicializa√ß√µes subsequentes: ~30 segundos
- Requer configura√ß√£o manual do backend

---

## Quest√µes de Uso

### Qual modelo de IA devo usar?

| Modelo | Melhor Para | Velocidade | Qualidade | Portugu√™s | Tamanho |
|--------|-------------|------------|-----------|-----------|---------|
| **llama3.1:8b** ‚≠ê | **Recomendado (PT-BR)** | Boa | Excelente | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 4.7GB |
| **gemma2:9b** | Melhor qualidade | M√©dia | Excelente | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 5.4GB |
| **qwen2.5:7b** | Velocidade | R√°pida | Muito Boa | ‚≠ê‚≠ê‚≠ê‚≠ê | 4.7GB |
| **mistral** | Uso geral | M√©dia | Boa | ‚≠ê‚≠ê‚≠ê | 4.1GB |
| **tinyllama** | Varreduras r√°pidas | Muito R√°pida | B√°sica | ‚≠ê‚≠ê | 1.1GB |

Comece com `llama3.1:8b` - oferece excelente suporte ao portugu√™s brasileiro.

### Posso usar m√∫ltiplos modelos?

Sim! Instale m√∫ltiplos modelos:

```bash
# Instalar modelos adicionais
docker exec NautiKube-ollama ollama pull gemma2:9b
docker exec NautiKube-ollama ollama pull mistral

# NautiKube v2 usa automaticamente o modelo dispon√≠vel
# Para K8sGPT, reconfigure o backend:
docker exec NautiKube-k8sgpt k8sgpt auth add --backend ollama --model gemma2:9b --baseurl http://localhost:11434
```

### Com que frequ√™ncia devo executar a an√°lise?

**Cronograma recomendado**:
- **Ap√≥s deployments**: Verificar problemas imediatamente
- **Diariamente**: Verifica√ß√µes rotineiras de sa√∫de
- **Antes de releases**: Validar estado do cluster
- **Quando alertas disparam**: Investigar causa raiz

### Posso analisar apenas recursos espec√≠ficos?

Sim! Use filtros:

**NautiKube v2.0**:
```bash
# Analisar apenas Pods
docker exec NautiKube NautiKube analyze --filter Pod --explain --language Portuguese

# Analisar apenas ConfigMaps
docker exec NautiKube NautiKube analyze --filter ConfigMap --explain --language Portuguese

# Namespace espec√≠fico
docker exec NautiKube NautiKube analyze -n production --explain --language Portuguese
```

**K8sGPT (legado)**:
```bash
# Com profile k8sgpt
docker exec NautiKube-k8sgpt k8sgpt analyze --filter=Pod --explain --language Portuguese

# Listar filtros dispon√≠veis
docker exec NautiKube-k8sgpt k8sgpt filters list
```

> üí° NautiKube v2 tem suporte nativo ao portugu√™s, mas voc√™ pode especificar `--language Portuguese` ou `--language English`.

### Que tipos de problemas ele pode detectar?

**NautiKube v2.0 detecta**:
- **Pods**: 
  - CrashLoopBackOff
  - ImagePullBackOff
  - ContainerStatusUnknown
  - Containers terminados
- **ConfigMaps**: 
  - ConfigMaps n√£o utilizados

**K8sGPT (legado) analisa**:
- **Pods**, **Services**, **Deployments**, **PVCs**, **Ingress**
- **StatefulSets**, **HPA**, **NetworkPolicies**
- E mais tipos de recursos

> üí° NautiKube v2 √© focado nos problemas mais comuns (Pods e ConfigMaps). Novos scanners podem ser adicionados facilmente.

---

## Quest√µes T√©cnicas

### Como funciona?

**NautiKube v2.0**:
1. CLI recebe comando `NautiKube analyze`
2. Scanner conecta √† API Kubernetes via client-go
3. Detecta problemas em Pods e ConfigMaps
4. Analyzer aplica filtros (se especificados)
5. Se `--explain`, envia para Ollama via HTTP
6. Ollama (llama3.1:8b) gera explica√ß√£o em portugu√™s
7. CLI exibe resultados formatados

**K8sGPT (legado)**:
1. K8sGPT escaneia cluster via API Kubernetes
2. Analisadores identificam problemas
3. Envia contexto para Ollama
4. LLM gera explica√ß√£o
5. Resultados exibidos

### Ele modifica meu cluster?

**N√£o!** NautiKube √© somente leitura. Ele:
- ‚úÖ L√™ o estado do cluster
- ‚úÖ Analisa configura√ß√µes
- ‚úÖ Gera relat√≥rios
- ‚ùå Nunca faz mudan√ßas
- ‚ùå Nunca deleta recursos
- ‚ùå Nunca aplica configura√ß√µes

### Quais permiss√µes ele precisa?

K8sGPT requer acesso **somente leitura** aos recursos do cluster. As mesmas permiss√µes dos comandos `kubectl get`.

### Posso executar em CI/CD?

Sim! Exemplo:

```yaml
# GitLab CI - NautiKube v2
k8s-analysis:
  script:
    - docker-compose up -d
    - docker exec NautiKube NautiKube analyze --explain --language Portuguese > report.txt
  artifacts:
    paths:
      - report.txt

# GitLab CI - K8sGPT legado
k8s-analysis-legacy:
  script:
    - docker-compose --profile k8sgpt up -d
    - docker exec NautiKube-k8sgpt k8sgpt analyze --explain --language Portuguese > report.txt
  artifacts:
    paths:
      - report.txt
```

### As an√°lises s√£o sempre em portugu√™s?

**NautiKube v2.0**: Suporte nativo ao portugu√™s! Basta usar `--language Portuguese` (ou omitir para ingl√™s).

```bash
# Portugu√™s (recomendado)
docker exec NautiKube NautiKube analyze --explain --language Portuguese

# Ingl√™s
docker exec NautiKube NautiKube analyze --explain --language English
```

**K8sGPT (legado)**: Requer flag `--language Portuguese` explicitamente.

**Idiomas suportados**: English, Portuguese

> ‚≠ê O modelo **llama3.1:8b** oferece excelente qualidade em portugu√™s brasileiro!

### Posso exportar resultados?

Sim! Redirecione a sa√≠da:

```bash
# NautiKube v2 - Salvar em arquivo
docker exec NautiKube NautiKube analyze --explain --language Portuguese > analysis.txt

# K8sGPT - JSON
docker exec NautiKube-k8sgpt k8sgpt analyze --explain --output json --language Portuguese > analysis.json
```

---

## Solu√ß√£o de Problemas

### Por que est√° lento?

**Poss√≠veis causas**:
1. **Modelo grande**: Tente `tinyllama` para respostas mais r√°pidas
2. **Muitos recursos**: Use filtros ou escopo de namespace
3. **RAM limitada**: Aloque mais mem√≥ria para o Docker
4. **Gargalo de CPU**: Feche outras aplica√ß√µes

**Otimiza√ß√£o**:
```bash
# Usar modelo menor
docker exec NautiKube-ollama ollama pull tinyllama

# Limitar escopo
docker exec NautiKube-k8sgpt k8sgpt analyze --namespace default --explain
docker exec NautiKube-k8sgpt k8sgpt analyze --filter=Pod --explain
```

### Diz "nenhum problema encontrado" mas sei que h√° problemas

1. **Verificar namespace**: Padr√£o √© todos os namespaces
   ```bash
   docker exec NautiKube-k8sgpt k8sgpt analyze --namespace seu-namespace --explain
   ```

2. **Tentar filtros diferentes**: Alguns problemas precisam de analisadores espec√≠ficos
   ```bash
   docker exec NautiKube-k8sgpt k8sgpt filters list
   docker exec NautiKube-k8sgpt k8sgpt analyze --filter=Pod --explain
   ```

3. **Verificar acesso ao cluster**:
   ```bash
   docker exec NautiKube-k8sgpt kubectl get pods --all-namespaces
   ```

### Ollama continua baixando modelos

Modelos s√£o armazenados em volumes Docker. Se voc√™ executar `docker-compose down -v`, modelos s√£o deletados.

**Preservar modelos**:
```bash
# Parar sem remover volumes
docker-compose down

# Ou apenas reiniciar
docker-compose restart
```

### Posso usar uma inst√¢ncia Ollama externa?

Sim! Modifique o `docker-compose.yml`:

```yaml
k8sgpt:
  environment:
    - OLLAMA_BASEURL=http://seu-servidor-ollama:11434
```

Ent√£o remova a defini√ß√£o do servi√ßo Ollama.

---

## Uso Avan√ßado

### Posso personalizar os analisadores do K8sGPT?

K8sGPT usa analisadores integrados. Para habilitar/desabilitar:

```bash
# Listar filtros dispon√≠veis
docker exec NautiKube-k8sgpt k8sgpt filters list

# Usar filtros espec√≠ficos
docker exec NautiKube-k8sgpt k8sgpt analyze --filter=Pod,Service --explain
```

### Posso usar um backend LLM diferente?

Sim! K8sGPT suporta:
- Ollama (local) - padr√£o
- OpenAI (nuvem)
- Azure OpenAI (nuvem)
- LocalAI (alternativa local)

Exemplo para OpenAI:
```bash
docker exec NautiKube-k8sgpt k8sgpt auth add \
  --backend openai \
  --model gpt-4 \
  --password SUA_API_KEY
```

### Como fa√ßo backup da minha configura√ß√£o?

```bash
# Backup dos modelos Ollama
docker run --rm \
  -v NautiKube-ollama-data:/data \
  -v ${PWD}:/backup \
  alpine tar czf /backup/ollama-backup.tar.gz /data

# Backup da config K8sGPT
docker run --rm \
  -v NautiKube-k8sgpt-config:/data \
  -v ${PWD}:/backup \
  alpine tar czf /backup/k8sgpt-backup.tar.gz /data
```

### Posso executar m√∫ltiplas inst√¢ncias?

Sim, mas altere os nomes dos cont√™ineres para evitar conflitos:

```bash
# No arquivo .env
CONTAINER_NAME_OLLAMA=NautiKube-ollama-2
CONTAINER_NAME_K8SGPT=NautiKube-k8sgpt-2
OLLAMA_PORT=11435
```

### Como atualizo para a vers√£o mais recente?

```bash
# Puxar c√≥digo mais recente
git pull origin main

# Reconstruir cont√™ineres
docker-compose build

# Reiniciar servi√ßos
docker-compose restart
```

---

## Performance & Otimiza√ß√£o

### Quanto espa√ßo em disco preciso?

- **Instala√ß√£o base**: ~500MB (cont√™ineres)
- **Por modelo**: 1-10GB dependendo do modelo
- **Logs**: ~100MB (cresce com o tempo)
- **Recomenda√ß√£o**: 20GB de espa√ßo livre

### Posso limitar o uso de recursos?

Sim! Edite o `docker-compose.yml`:

```yaml
services:
  ollama:
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
```

### Qual modelo √© mais r√°pido?

Ranking de velocidade (mais r√°pido para mais lento):
1. `tinyllama` - ~2-5 segundos por explica√ß√£o
2. `gemma:7b` - ~5-10 segundos por explica√ß√£o
3. `mistral` - ~8-15 segundos por explica√ß√£o
4. `llama2:13b` - ~15-30 segundos por explica√ß√£o

### Posso usar acelera√ß√£o GPU?

Sim, se voc√™ tiver GPU NVIDIA:

1. Instale o [NVIDIA Container Toolkit](https://github.com/NVIDIA/nvidia-docker)
2. Modifique o `docker-compose.yml`:

```yaml
services:
  ollama:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

---

## Seguran√ßa & Privacidade

### Meu kubeconfig est√° seguro?

Sim:
- Montado como **somente leitura** no cont√™iner
- Nunca modificado ou exposto
- Usado apenas para acesso √† API
- Cont√™iner est√° isolado

### Quais dados s√£o coletados?

**Nenhum!** NautiKube:
- ‚ùå Sem telemetria
- ‚ùå Sem analytics
- ‚ùå Sem conex√µes externas (exceto downloads de modelos)
- ‚ùå Sem compartilhamento de dados

### Posso usar em produ√ß√£o?

Sim, mas:
- ‚úÖ √â somente leitura (seguro)
- ‚úÖ Sem modifica√ß√µes no cluster
- ‚ö†Ô∏è Garanta recursos adequados
- ‚ö†Ô∏è Teste em dev/staging primeiro
- ‚ö†Ô∏è Monitore o uso de recursos

### Devo fazer commit do meu arquivo .env?

**N√ÉO!** O arquivo `.env` pode conter informa√ß√µes sens√≠veis. J√° est√° no `.gitignore`.

---

## Contribuindo & Suporte

### Como posso contribuir?

Veja [CONTRIBUTING.md](../CONTRIBUTING.md) para:
- Reportar bugs
- Sugerir funcionalidades
- Enviar pull requests
- Melhorar a documenta√ß√£o

### Onde reporto bugs?

Abra uma issue no [GitHub Issues](https://github.com/jorgegabrielti/NautiKube/issues) com:
- SO e vers√£o do Docker
- Sa√≠da de `docker-compose ps`
- Passos para reproduzir
- Mensagens de erro/logs

### Posso solicitar novas funcionalidades?

Sim! Abra uma GitHub Issue com:
- Descri√ß√£o da funcionalidade
- Caso de uso
- Comportamento esperado
- Exemplo de uso

### Como obtenho ajuda?

1. Verifique este FAQ
2. Leia [TROUBLESHOOTING.md](TROUBLESHOOTING.md)
3. Pesquise [issues existentes](https://github.com/jorgegabrielti/NautiKube/issues)
4. Abra uma nova issue
5. Participe das discuss√µes

---

## Roadmap & Futuro

### O que est√° planejado para vers√µes futuras?

- Dashboard Web UI
- Integra√ß√µes Slack/Teams
- Plugins de analisador customizados
- Rastreamento de an√°lise hist√≥rica
- Modo operador Kubernetes
- Suporte multi-cluster

### Posso patrocinar o projeto?

Ainda n√£o, mas fique ligado! Enquanto isso, contribui√ß√µes e estrelas no GitHub s√£o apreciadas! ‚≠ê

---

## Compara√ß√£o com Outras Ferramentas

### NautiKube vs kubectl

- **kubectl**: Comandos de baixo n√≠vel, interpreta√ß√£o manual
- **NautiKube**: An√°lise automatizada com explica√ß√µes de IA

### NautiKube vs K9s

- **K9s**: TUI interativa para gerenciamento de cluster
- **NautiKube**: Detec√ß√£o automatizada de problemas com IA

### NautiKube vs Lens

- **Lens**: IDE desktop GUI para Kubernetes
- **NautiKube**: Ferramenta CLI com an√°lise de IA

### NautiKube vs Prometheus/Grafana

- **Prometheus/Grafana**: M√©tricas e monitoramento
- **NautiKube**: Detec√ß√£o e explica√ß√£o de problemas

**Eles se complementam!** Use NautiKube para diagn√≥sticos junto com suas ferramentas existentes.

---

## Recursos Adicionais

- üìñ [Documenta√ß√£o de Arquitetura](ARCHITECTURE.md)
- üîß [Guia de Solu√ß√£o de Problemas](TROUBLESHOOTING.md)
- ü§ù [Diretrizes de Contribui√ß√£o](../CONTRIBUTING.md)
- üìù [Hist√≥rico de Mudan√ßas](../CHANGELOG.md)
- üêô [Reposit√≥rio GitHub](https://github.com/jorgegabrielti/NautiKube)
- üîó [Documenta√ß√£o K8sGPT](https://docs.k8sgpt.ai/)
- ü¶ô [Documenta√ß√£o Ollama](https://github.com/ollama/ollama)

---

**N√£o encontrou sua resposta?** Abra uma issue no GitHub!
