# Arquitetura

## Vis√£o Geral do Sistema

NautiKube v2.0 √© uma solu√ß√£o containerizada **pr√≥pria** desenvolvida em Go que analisa clusters Kubernetes e fornece explica√ß√µes via IA local (Ollama). A solu√ß√£o substitui o K8sGPT por um engine customizado mais leve e r√°pido.

## Componentes Principais

### üéØ NautiKube v2 (Padr√£o)

**Engine Pr√≥prio em Go**

#### CLI (`cmd/NautiKube/main.go`)
- Framework: Cobra v1.8.0
- Comandos: `analyze`, `version`
- Flags: `--namespace`, `--filter`, `--explain`, `--language`
- Entry point da aplica√ß√£o

#### Scanner (`internal/scanner/scanner.go`)
- Usa `client-go` para acessar API Kubernetes
- Detecta problemas em Pods:
  - CrashLoopBackOff
  - ImagePullBackOff
  - ContainerStatusUnknown
  - Containers terminados
- Detecta ConfigMaps n√£o utilizados
- Extens√≠vel para novos tipos de recursos

#### Analyzer (`internal/analyzer/analyzer.go`)
- Coordena scanning e an√°lise
- Aplica filtros por tipo de recurso
- Integra com Ollama Client para explica√ß√µes
- Retorna lista estruturada de problemas

#### Ollama Client (`internal/ollama/client.go`)
- Cliente HTTP para API Ollama
- Prompts otimizados para portugu√™s
- Timeout de 120s para gera√ß√£o
- Health check do servi√ßo Ollama
- URL: `http://host.docker.internal:11434`

#### Types (`pkg/types/types.go`)
- Estruturas compartilhadas
- `Problem`: representa problema detectado
- `AnalyzeOptions`: op√ß√µes de an√°lise
- `OllamaRequest/Response`: comunica√ß√£o com Ollama

**Recursos Principais**:
- üöÄ Startup <10s (3x mais r√°pido que K8sGPT)
- üíæ Imagem ~80MB (60% menor)
- üîß Configura√ß√£o autom√°tica (zero setup)
- üáßüá∑ Suporte nativo a portugu√™s
- ‚ö° Performance otimizada

### üîÑ K8sGPT (Modo Legado via Profile)

Dispon√≠vel com `docker-compose --profile k8sgpt up -d` para compatibilidade.

**Fun√ß√£o**: An√°lise original via ferramenta externa

**Recursos**:
- An√°lise completa de recursos K8s
- Requer configura√ß√£o manual de backend
- Imagem maior (~200MB)
- Mantido para retrocompatibilidade

### ü§ñ Cont√™iner Ollama (Compartilhado)

**Fun√ß√£o**: Executa modelos LLM localmente

**Recursos**:
- API REST na porta 11434
- Armazenamento persistente de modelos
- Modelo padr√£o: llama3.1:8b (4.7GB)
- Compartilhado entre NautiKube e K8sGPT

## Fluxo de Dados (v2.0)

1. **Solicita√ß√£o**: `NautiKube analyze --explain` ‚Üí CLI Cobra
2. **Inicializa√ß√£o**: CLI ‚Üí Analyzer ‚Üí Scanner (client-go)
3. **Scanning**: Scanner ‚Üí API Kubernetes ‚Üí Lista de recursos
4. **An√°lise**: Scanner ‚Üí Detecta problemas ‚Üí Lista de Problems
5. **Filtragem**: Analyzer ‚Üí Aplica filtros ‚Üí Problems filtrados
6. **Explica√ß√£o IA** (opcional): 
   - Analyzer ‚Üí Ollama Client ‚Üí HTTP POST
   - Ollama ‚Üí llama3.1:8b ‚Üí Explica√ß√£o em PT-BR
   - Ollama Client ‚Üí Analyzer ‚Üí Problem com explica√ß√£o
7. **Output**: CLI ‚Üí Formata ‚Üí Console ‚Üí Usu√°rio

## Estrutura do C√≥digo (v2.0)

```
NautiKube/
‚îú‚îÄ‚îÄ cmd/
‚îÇ   ‚îî‚îÄ‚îÄ NautiKube/
‚îÇ       ‚îî‚îÄ‚îÄ main.go              # Entry point, CLI Cobra
‚îú‚îÄ‚îÄ internal/
‚îÇ   ‚îú‚îÄ‚îÄ scanner/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scanner.go           # K8s resource scanner
‚îÇ   ‚îú‚îÄ‚îÄ analyzer/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ analyzer.go          # Analysis coordinator
‚îÇ   ‚îî‚îÄ‚îÄ ollama/
‚îÇ       ‚îî‚îÄ‚îÄ client.go            # Ollama HTTP client
‚îú‚îÄ‚îÄ pkg/
‚îÇ   ‚îî‚îÄ‚îÄ types/
‚îÇ       ‚îî‚îÄ‚îÄ types.go             # Shared structures
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.NautiKube   # Multi-stage build
‚îÇ   ‚îî‚îÄ‚îÄ entrypoint-NautiKube.sh # Container init
‚îú‚îÄ‚îÄ go.mod / go.sum              # Dependencies
‚îî‚îÄ‚îÄ docker-compose.yml           # Orchestration
```

## Arquitetura de Rede

### Host Network Mode

```yaml
network_mode: host
```

**Usado por**: NautiKube, K8sGPT

**Vantagens**:
- Acesso direto ao cluster K8s local
- Acesso ao Ollama via `host.docker.internal:11434`
- Performance otimizada
- Configura√ß√£o simplificada

**Considera√ß√µes**:
- Necess√°rio para acesso ao kubeconfig local
- Permite comunica√ß√£o entre containers via host

## Gerenciamento de Volumes

### Volumes Persistentes

1. **NautiKube-ollama-data**: 
   - Armazena modelos LLM (~4.7GB por modelo)
   - Compartilhado entre vers√µes
   
2. **NautiKube-k8sgpt-config** (legacy):
   - Configura√ß√£o K8sGPT
   - Apenas quando usando profile k8sgpt

3. **~/.kube/config**: 
   - Montado read-only em ambos containers
   - Modificado automaticamente pelo entrypoint

## Considera√ß√µes de Seguran√ßa

### Acesso ao Kubeconfig
- Montado como **read-only**
- Modificado temporariamente em `/root/.kube/config_mod`
- Nunca altera arquivo original
- Isolado dentro do container

### Seguran√ßa de Rede
- Network mode host necess√°rio para acesso ao cluster
- Ollama acess√≠vel apenas via containers (n√£o exposto)
- Sem exposi√ß√£o de portas externas
- Comunica√ß√£o via Docker internal networking

### Privacidade de Dados
- ‚úÖ 100% local - nenhum dado sai da m√°quina
- ‚úÖ Sem telemetria ou analytics
- ‚úÖ Modelos LLM rodando offline
- ‚úÖ Logs apenas em stdout/stderr

## Performance e Escalabilidade

### Requisitos de Recursos (v2.0)

**M√≠nimo (NautiKube)**:
- 1 n√∫cleo CPU
- 2GB RAM
- 5GB disco (modelo llama3.1:8b)

**Recomendado**:
- 2-4 n√∫cleos CPU
- 4-8GB RAM (Ollama usa ~4GB)
- 10GB disco (m√∫ltiplos modelos)

**Compara√ß√£o v1 vs v2**:
| M√©trica | K8sGPT (v1) | NautiKube (v2) |
|---------|-------------|------------------|
| Imagem Docker | ~200MB | ~80MB |
| RAM em execu√ß√£o | ~150MB | ~50MB |
| Startup | ~30s | <10s |
| Scan 50 Pods | ~5s | ~2s |

**Recomendado**:
- 4+ n√∫cleos de CPU
- 8GB+ RAM
- 20GB+ disco

### Performance do Modelo

| Modelo | Tamanho | Velocidade | Qualidade |
|--------|---------|------------|-----------|
| tinyllama | 1.1GB | R√°pido | B√°sica |
| gemma:7b | 4.8GB | M√©dio | Boa |
| mistral | 4.1GB | M√©dio | Boa |
| llama2:13b | 7.4GB | Lento | Excelente |

### Dicas de Otimiza√ß√£o

1. Use modelos menores para an√°lises r√°pidas
2. Limite o escopo com filtros e namespaces
3. Aloque mais recursos para modelos maiores
4. Use acelera√ß√£o GPU quando dispon√≠vel

## Pontos de Integra√ß√£o

### Integra√ß√£o CI/CD

```yaml
# GitLab CI
k8s-analysis:
  script:
    - docker-compose up -d
    - docker exec NautiKube-k8sgpt k8sgpt analyze --explain > report.txt
  artifacts:
    paths:
      - report.txt
```

### Integra√ß√£o de Monitoramento

NautiKube complementa ferramentas de monitoramento existentes:
- Prometheus/Grafana: m√©tricas
- NautiKube: detec√ß√£o e explica√ß√£o de problemas

### Integra√ß√£o de Alertas

Use NautiKube em resposta a alertas para diagn√≥stico automatizado.

## Solu√ß√£o de Problemas de Arquitetura

Para problemas comuns, consulte [TROUBLESHOOTING.md](TROUBLESHOOTING.md).
